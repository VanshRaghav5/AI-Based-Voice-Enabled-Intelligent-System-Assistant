<<<<<<< HEAD
# ðŸ§  AI-Based Voice-Enabled Intelligent System (Windows)

A modular, production-ready AI voice assistant for Windows desktop automation.

This system combines:

- ðŸŽ™ Offline Speech-to-Text (Whisper)
- ðŸ”Š Offline Text-to-Speech (Piper)
- âš™ Agent-Based Automation Engine
- ðŸ§© Tool Registry & Executor Architecture
- ðŸ–¥ System + File + Application Automation

---

## ðŸš€ Architecture Overview

Voice Input (Whisper - GPU)
â†“
Audio Pipeline
â†“
Assistant Controller
â†“
Agent / Planner
â†“
Tool Registry
â†“
Automation Tool Execution
â†“
Voice Response (Piper TTS)


The system is fully modular and designed for scalability and future LLM integration.

---

## ðŸ“ Project Structure

backend/
â”‚
â”œâ”€â”€ voice_engine/ # STT, TTS, audio pipeline
â”‚ â”œâ”€â”€ input/
â”‚ â”œâ”€â”€ stt/
â”‚ â”œâ”€â”€ tts/
â”‚ â””â”€â”€ audio_pipeline.py
â”‚
â”œâ”€â”€ automation/ # All automation tools
â”‚ â”œâ”€â”€ base_tool.py
â”‚ â”œâ”€â”€ system/
â”‚ â”œâ”€â”€ file/
â”‚ â”œâ”€â”€ whatsapp_desktop.py
â”‚ â””â”€â”€ ...
â”‚
â”œâ”€â”€ core/ # Agent, Executor, Tool Registry
â”‚ â”œâ”€â”€ assistant_controller.py
â”‚ â”œâ”€â”€ agent.py
â”‚ â”œâ”€â”€ executor.py
â”‚ â””â”€â”€ tool_registry.py
â”‚
â”œâ”€â”€ config/ # Logger & settings
â”‚
â””â”€â”€ data/ # Runtime storage (ignored in git)


---

## ðŸŽ™ Voice Capabilities

### âœ… Speech-to-Text
- OpenAI Whisper (GPU enabled)
- English-only transcription
- Deterministic configuration
- Push-to-talk support

### âœ… Text-to-Speech
- Piper TTS (offline)
- Custom tuning parameters:
  - `length_scale`
  - `noise_scale`
  - `noise_w`
- Runtime audio cleanup

---

## âš™ Automation Capabilities

### ðŸ–¥ System Control
- Lock laptop
- Shutdown
- Restart
- Volume up/down
- Mute

### ðŸ“‚ File Operations
- Open file
- Create file
- Delete file
- Move file
- Create folder
- Delete folder
- Search file

### ðŸ“± Application Automation
- Open WhatsApp Desktop
- Send WhatsApp message
- Launch applications
- Browser control

All automation tools follow:

BaseTool â†’ ToolRegistry â†’ Executor


This allows easy addition of new tools without modifying core logic.

---

## ðŸ§© Agent-Based Execution Model

Each command is converted into:

python
ToolCall(
    name="file.open",
    args={"path": "C:/Users/..."}
)
Then executed through:

Executor â†’ ToolRegistry â†’ Tool.execute()
No hardcoded spaghetti IF-ELSE chains.

ðŸ›  Setup Instructions
1ï¸âƒ£ Clone Repository
git clone https://github.com/your-repo-link
cd AI-Based-Voice-Enabled-Intelligent-System-Assistant
2ï¸âƒ£ Create Virtual Environment
python -m venv venv
venv\Scripts\activate
3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt
4ï¸âƒ£ Run Assistant
python app.py
ðŸŽ® Usage
When running:

Hold SPACE â†’ Voice input

Press CTRL + T â†’ Text input

Say/type:

"Open WhatsApp"

"Lock my laptop"

"Shutdown system"

"Create folder test in documents"

"Send hello to Swayam on WhatsApp"

Say:

exit
to terminate assistant.

ðŸ§  Design Principles
Fully offline for core features

Modular & production-ready

Tool-based architecture

Thread-safe ready

LLM-integration ready

Clean separation of concerns

ðŸ”’ Security
No cloud dependency for automation

No remote command execution

All operations run locally on Windows

ðŸ— Future Improvements
LLM-based intent parsing

GUI dashboard

Context memory

Multi-step planning

Advanced permission system

ðŸ‘¨â€ðŸ’» Authors
Voice & Automation Core: Vansh Raghav

LLM Integration: Team Member

UI & Deployment: Team Member

ðŸ“Œ Status
Production-ready local automation system with scalable agent architecture.


---

# ðŸ”¥ This README Is:

- Clean
- Professional
- Evaluator-friendly
- Architecture-focused
- Industry-level structured

---

If you want, I can also:

- Create a **high-impact GitHub landing header**
- Add architecture diagram image
- Make it more research-paper style
- Or make it startup-style product README**

Tell me the vibe you want.
=======
# Ollama Intent Engine

A local intent classification and entity extraction system powered by a custom Ollama model (LLaMA 3 8B). It parses natural-language user commands into structured JSON, then routes them to the appropriate executor via a planner.

## Architecture

```
User Command
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Prompt  â”‚â”€â”€â”€â–¶â”‚  Ollama  â”‚â”€â”€â”€â–¶â”‚ Planner  â”‚â”€â”€â”€â–¶ Executor
â”‚ Template â”‚    â”‚  Model   â”‚    â”‚ (Router) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                              â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
                              â”‚  Entities   â”‚
                              â”‚  Validation â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

1. **Prompt** (`prompt.txt`) â€” Instructs the LLM to classify the user's command into an intent and extract entities.
2. **Model** (`Modelfile`) â€” A deterministic Ollama model built on `llama3:8b` with low temperature/top-p to ensure consistent, JSON-only output.
3. **Planner** (`planner.py`) â€” Validates the LLM response (confidence, entities) and dispatches to the correct executor function.

## Files

| File | Purpose |
| --- | --- |
| `Modelfile` | Ollama model definition â€” base model, parameters, and system prompts |
| `prompt.txt` | Prompt template sent to the model with the user's command |
| `entities.json` | Schema defining required/optional entities per intent |
| `intent.md` | Frozen intent taxonomy â€” the single source of truth for allowed intents |
| `planner.py` | Decision engine that validates and routes LLM output to executors |

## Supported Intents

| Intent | Required Entities | Optional Entities | Example |
| --- | --- | --- | --- |
| `open_application` | `application` | â€” | *"Open Spotify"* |
| `open_application_and_navigate` | `application`, `url` | â€” | *"Open Chrome and go to YouTube"* |
| `play_music` | `song` | `artist` | *"Play Bohemian Rhapsody by Queen"* |
| `get_weather` | `location` | â€” | *"What's the weather in Tokyo?"* |
| `get_fact` | `subject`, `fact_type` | â€” | *"Tell me a fun fact about Mars"* |
| `get_definition` | `topic` | â€” | *"Define machine learning"* |
| `get_tips` | `topic` | â€” | *"Give me tips on public speaking"* |
| `unknown` | â€” | â€” | Anything that doesn't match above |

## Model Configuration

Defined in `Modelfile`:

| Parameter | Value | Reason |
| --- | --- | --- |
| `temperature` | 0.0 | Deterministic output â€” no creativity needed |
| `top_p` | 0.5 | Narrow token sampling for consistency |
| `repeat_penalty` | 1.2 | Prevents repetitive tokens in output |
| `num_ctx` | 2048 | Context window size |

The model is constrained via system prompts to:

- Output **only** valid JSON (no markdown, no explanations)
- Cap confidence between **0.0 and 0.95** (never 1.0)

## LLM Output Schema

The model returns a single JSON object:

```json
{
  "intent": "open_application_and_navigate",
  "entities": {
    "application": "chrome",
    "url": "https://www.youtube.com"
  },
  "confidence": 0.92
}
```

## Planner Logic

`planner.py` processes the LLM output through these steps:

1. **Confidence check** â€” If `confidence < 0.7`, ask for clarification.
2. **Unknown intent** â€” If intent is `unknown`, ask for clarification.
3. **Entity validation** â€” Checks that all required entities (defined in `entities.json`) are present.
4. **Dispatch** â€” Routes to the matching executor function (e.g., `open_application()`, `play_music()`).

## Setup

### Prerequisites

- [Ollama](https://ollama.com/) installed and running locally

### Create the Model

```bash
ollama create intent-engine -f Modelfile
```

### Run a Query

```bash
ollama run intent-engine "Open Chrome and go to YouTube"
```

The model will return a JSON object that can be passed directly to `planner.plan()`.

## Adding a New Intent

1. Add the intent name to `intent.md`.
2. Define its required/optional entities in `entities.json`.
3. Add a handler branch in `planner.py`.
4. Update the `ALLOWED INTENTS` list in `prompt.txt`.
5. Implement the corresponding executor function.
>>>>>>> swayam_ai
